{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba483f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.29.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chargement des librairies\n",
    "import datetime\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotly\n",
    "import plotly\n",
    "# import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# Warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c19ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_tables(directory_path='', verbose=True):\n",
    "    '''\n",
    "    Function to load all the tables required\n",
    "    Input:\n",
    "        directory_path: str, default = ''\n",
    "            Path of directory in which tables are stored in\n",
    "        verbose: bool, default = True\n",
    "            Whether to keep verbosity or not\n",
    "\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Chargement des jeux de donnees...\")\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        start = datetime.now()\n",
    "\n",
    "    application_train = pd.read_csv(directory_path + 'application_train.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier application_train.csv chargé -> dataframe : application_train\")\n",
    "\n",
    "    application_test = pd.read_csv(directory_path + 'application_test.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier application_test.csv chargé -> dataframe : application_test\")\n",
    "\n",
    "    bureau = pd.read_csv(directory_path + 'bureau.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier bureau.csv chargé -> dataframe : bureau\")\n",
    "\n",
    "    bureau_balance = pd.read_csv(directory_path + 'bureau_balance.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier bureau_balance.csv chargé -> dataframe : bureau_balance\")\n",
    "\n",
    "    cc_balance = pd.read_csv(directory_path + 'credit_card_balance.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier credit_card_balance.csv chargé -> dataframe : cc_balance\")\n",
    "\n",
    "    installments_payments = pd.read_csv(\n",
    "        directory_path + 'installments_payments.csv')\n",
    "    if verbose:\n",
    "        print(\n",
    "            \"Fichier installments_payments.csv chargé -> dataframe : installments_payments\")\n",
    "\n",
    "    POS_CASH_balance = pd.read_csv(directory_path + 'POS_CASH_balance.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier POS_CASH_balance.csv chargé -> dataframe : POS_CASH_balance\")\n",
    "\n",
    "    HomeCredit_columns_description = pd.read_csv(\n",
    "        directory_path +\n",
    "        'HomeCredit_columns_description.csv',\n",
    "        encoding='cp1252')\n",
    "    del HomeCredit_columns_description['Unnamed: 0']\n",
    "    if verbose:\n",
    "        print(\"Fichier HomeCredit_columns_description.csv chargé -> dataframe : HomeCredit_columns_description\")\n",
    "\n",
    "    previous_application = pd.read_csv(\n",
    "        directory_path + 'previous_application.csv')\n",
    "    if verbose:\n",
    "        print(\"Fichier previous_application.csv chargé -> dataframe : previous_application\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        print(\n",
    "            f'Chargement des 9 jeux de donnees terminée en {datetime.now() - start} secondes')\n",
    "\n",
    "    return application_train, application_test, bureau, bureau_balance, \\\n",
    "        cc_balance, installments_payments, POS_CASH_balance, previous_application, \\\n",
    "        HomeCredit_columns_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9afbc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_types_variables(df_work, types, type_par_var, graph):\n",
    "    \"\"\" Permet un aperçu du type des variables\n",
    "    Parameters\n",
    "    ----------\n",
    "    @param IN : df_work : dataframe, obligatoire\n",
    "                types : Si True lance dtypes, obligatoire\n",
    "                type_par_var : Si True affiche tableau des types de\n",
    "                               chaque variable, obligatoire\n",
    "                graph : Si True affiche pieplot de répartition des types\n",
    "    @param OUT :None.\n",
    "    \"\"\"\n",
    "\n",
    "    if types:\n",
    "        # 1. Type des variables\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"Type de variable pour chacune des variables\\n\")\n",
    "        display(df_work.dtypes)\n",
    "\n",
    "    if type_par_var:\n",
    "        # 2. Compter les types de variables\n",
    "        #print(\"Répartition des types de variable\\n\")\n",
    "        values = df_work.dtypes.value_counts()\n",
    "        nb_tot = values.sum()\n",
    "        percentage = round((100 * values / nb_tot), 2)\n",
    "        table = pd.concat([values, percentage], axis=1)\n",
    "        table.columns = [\n",
    "            'Nombre par type de variable',\n",
    "            '% des types de variable']\n",
    "        display(table[table['Nombre par type de variable'] != 0]\n",
    "                .sort_values('% des types de variable', ascending=False)\n",
    "                .style.background_gradient('seismic'))\n",
    "\n",
    "    if graph:\n",
    "        # 3. Schéma des types de variable\n",
    "        # print(\"\\n----------------------------------------------------------\")\n",
    "        #print(\"Répartition schématique des types de variable \\n\")\n",
    "        # Répartition des types de variables\n",
    "        df_work.dtypes.value_counts().plot.pie(autopct='%1.1f%%')\n",
    "        plt.ylabel('')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd1cf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data, verbose=True):\n",
    "    # source: https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
    "    '''\n",
    "    This function is used to reduce the memory usage by converting the datatypes of a pandas\n",
    "    DataFrame withing required limits.\n",
    "    '''\n",
    "\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('-' * 79)\n",
    "        print('Memory usage du dataframe: {:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "\n",
    "        #  Float et int\n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(\n",
    "                        np.int8).min and c_max < np.iinfo(\n",
    "                        np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(\n",
    "                        np.float16).min and c_max < np.finfo(\n",
    "                        np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "        # # Boolean : pas à faire car pour machine learning il faut des int 0/1\n",
    "        # et pas False/True\n",
    "        # if list(data[col].unique()) == [0, 1] or list(data[col].unique()) == [1, 0]:\n",
    "        #     data[col] = data[col].astype(bool)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage après optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Diminution de {:.1f}%'.format(\n",
    "            100 * (start_mem - end_mem) / start_mem))\n",
    "        print('-' * 79)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d05654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traduire_valeurs_variable(dataframe, colonne_a_traduire, dictionnaire):\n",
    "    \"\"\"\n",
    "    Traduire les valeurs de la colonne du dataframe transmis par la valeur du dictionnaire\n",
    "    ----------\n",
    "    @param IN : dataframe : DataFrame, obligatoire\n",
    "                colonne_a_traduire : colonne dont on veut traduire les valeurs obligatoire\n",
    "                dictionnaire :dictionnaire clé=à remplacer,\n",
    "                              valeur = le texte de remplacement oblgatoire\n",
    "    @param OUT :None\n",
    "    \"\"\"\n",
    "    for cle, valeur in dictionnaire.items():\n",
    "        dataframe[colonne_a_traduire] = dataframe[colonne_a_traduire].replace(\n",
    "            cle, valeur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3f0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values(df_work, pourcentage, affiche_heatmap, retour=False):\n",
    "    \"\"\"Indicateurs sur les variables manquantes\n",
    "       @param in : df_work dataframe obligatoire\n",
    "                   pourcentage : boolean si True affiche le nombre heatmap\n",
    "                   affiche_heatmap : boolean si True affiche la heatmap\n",
    "       @param out : none\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Nombre de valeurs manquantes totales\n",
    "    nb_nan_tot = df_work.isna().sum().sum()\n",
    "    nb_donnees_tot = np.product(df_work.shape)\n",
    "    pourc_nan_tot = round((nb_nan_tot / nb_donnees_tot) * 100, 2)\n",
    "    print(\n",
    "        f'Valeurs manquantes : {nb_nan_tot} NaN pour {nb_donnees_tot} données ({pourc_nan_tot} %)')\n",
    "\n",
    "    if pourcentage:\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"Nombre et pourcentage de valeurs manquantes par variable\\n\")\n",
    "        # 2. Visualisation du nombre et du pourcentage de valeurs manquantes\n",
    "        # par variable\n",
    "        values = df_work.isnull().sum()\n",
    "        percentage = 100 * values / len(df_work)\n",
    "        table = pd.concat([values, percentage.round(2)], axis=1)\n",
    "        table.columns = [\n",
    "            'Nombres de valeurs manquantes',\n",
    "            '% de valeurs manquantes']\n",
    "        display(table[table['Nombres de valeurs manquantes'] != 0]\n",
    "                .sort_values('% de valeurs manquantes', ascending=False)\n",
    "                .style.background_gradient('seismic'))\n",
    "\n",
    "    if affiche_heatmap:\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(\"Heatmap de visualisation des valeurs manquantes\")\n",
    "        # 3. Heatmap de visualisation des valeurs manquantes\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        sns.heatmap(df_work.isna(), cbar=False)\n",
    "        plt.show()\n",
    "\n",
    "    if retour:\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c24b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_values_sup_seuil(df_work, seuil=90):\n",
    "    \"\"\"Retourne les variables qui ont plus que seuil% de valeurs manquantes.\n",
    "       @param in : df_work dataframe obligatoire\n",
    "                   seuil : seuil (90% par défaut)\n",
    "       @param out : cols_nan_a_suppr : liste des variables à supprimer\n",
    "    \"\"\"\n",
    "\n",
    "    values = df_work.isnull().sum()\n",
    "    percentage = 100 * values / len(df_work)\n",
    "    table = pd.concat([values, percentage.round(2)], axis=1)\n",
    "    table.columns = [\n",
    "            'Nombres de valeurs manquantes',\n",
    "            '% de valeurs manquantes']\n",
    "\n",
    "    # Liste des variables ayant plus de 90% de valeurs manquantes\n",
    "    cols_nan_a_suppr = table[table['% de valeurs manquantes'] > 90].index \\\n",
    "        .to_list()\n",
    "    nbr_cols_a_suppr = len(cols_nan_a_suppr)\n",
    "    if nbr_cols_a_suppr > 0:\n",
    "        print(f'{df_work.name} - Nombre de variables à supprimer : {len(cols_nan_a_suppr)}\\n')\n",
    "        display(cols_nan_a_suppr)\n",
    "    else:\n",
    "        print(f'{df_work.name} - Aucune variable à supprimer')\n",
    "\n",
    "    return cols_nan_a_suppr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b60261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_application(data):\n",
    "    '''\n",
    "    FEATURE ENGINEERING : création de nouvelles variables.\n",
    "    Extrait de : https://github.com/rishabhrao1997/Home-Credit-Default-Risk\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dataframe pour ajout de nouvelles variables, obligatoire.\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    '''\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables de revenu, de rente et de crédit :  ratio / différence\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Ratio : Montant du crédit du prêt / Revenu du demandeur\n",
    "    data['CREDIT_INCOME_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    # Ratio : Montant du crédit du prêt / Annuité de prêt\n",
    "    data['CREDIT_ANNUITY_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['AMT_ANNUITY'] + 0.00001)\n",
    "    # Ratio : Annuité de prêt / Revenu du demandeur\n",
    "    data['ANNUITY_INCOME_RATIO'] = data['AMT_ANNUITY'] / \\\n",
    "        (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    # Différence : Revenu du demandeur - Annuité de prêt\n",
    "    data['INCOME_ANNUITY_DIFF'] = data['AMT_INCOME_TOTAL'] - \\\n",
    "        data['AMT_ANNUITY']\n",
    "    # Ratio : Montant du crédit du prêt / prix des biens pour lesquels le prêt est accordé\n",
    "    # Crédit est supérieur au prix des biens ?\n",
    "    data['CREDIT_GOODS_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['AMT_GOODS_PRICE'] + 0.00001)\n",
    "    # Différence : Revenu du demandeur - prix des biens pour lesquels le prêt\n",
    "    # est accordé\n",
    "    data['INCOME_GOODS_DIFF'] = data['AMT_INCOME_TOTAL'] / \\\n",
    "        data['AMT_GOODS_PRICE']\n",
    "    # Ratio : Annuité de prêt / Âge du demandeur au moment de la demande\n",
    "    data['INCOME_AGE_RATIO'] = data['AMT_INCOME_TOTAL'] / (\n",
    "        data['DAYS_BIRTH'] + 0.00001)\n",
    "    # Ratio : Montant du crédit du prêt / Âge du demandeur au moment de la\n",
    "    # demande\n",
    "    data['CREDIT_AGE_RATIO'] = data['AMT_CREDIT'] / (\n",
    "        data['DAYS_BIRTH'] + 0.00001)\n",
    "    # Ratio : Revenu du demandeur / Score normalisé de la source de données\n",
    "    # externe 3\n",
    "    data['INCOME_EXT_RATIO'] = data['AMT_INCOME_TOTAL'] / \\\n",
    "        (data['EXT_SOURCE_3'] + 0.00001)\n",
    "    # Ratio : Montant du crédit du prêt / Score normalisé de la source de\n",
    "    # données externe\n",
    "    data['CREDIT_EXT_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['EXT_SOURCE_3'] + 0.00001)\n",
    "    # Multiplication : Revenu du demandeur\n",
    "    #                  * heure à laquelle le demandeur à fait sa demande de prêt\n",
    "    data['HOUR_PROCESS_CREDIT_MUL'] = data['AMT_CREDIT'] * \\\n",
    "        data['HOUR_APPR_PROCESS_START']\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur l'âge\n",
    "    # -----------------------------------------------------------------------\n",
    "    # YEARS_BIRTH - Âge du demandeur au moment de la demande DAYS_BIRTH en\n",
    "    # années\n",
    "    data['YEARS_BIRTH'] = data['DAYS_BIRTH'] * -1 / 365\n",
    "    # Différence : Âge du demandeur - Ancienneté dans l'emploi à date demande\n",
    "    data['AGE_EMPLOYED_DIFF'] = data['DAYS_BIRTH'] - data['DAYS_EMPLOYED']\n",
    "    # Ratio : Ancienneté dans l'emploi à date demande / Âge du demandeur\n",
    "    data['EMPLOYED_AGE_RATIO'] = data['DAYS_EMPLOYED'] / \\\n",
    "        (data['DAYS_BIRTH'] + 0.00001)\n",
    "    # Ratio : nombre de jours avant la demande où le demandeur a changé de téléphone \\\n",
    "    #         äge du client\n",
    "    data['LAST_PHONE_BIRTH_RATIO'] = data[\n",
    "        'DAYS_LAST_PHONE_CHANGE'] / (data['DAYS_BIRTH'] + 0.00001)\n",
    "    # Ratio : nombre de jours avant la demande où le demandeur a changé de téléphone \\\n",
    "    #         ancienneté dans l'emploi\n",
    "    data['LAST_PHONE_EMPLOYED_RATIO'] = data[\n",
    "        'DAYS_LAST_PHONE_CHANGE'] / (data['DAYS_EMPLOYED'] + 0.00001)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur la voiture\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Différence : Âge de la voiture du demandeur -  Ancienneté dans l'emploi\n",
    "    # à date demande\n",
    "    data['CAR_EMPLOYED_DIFF'] = data['OWN_CAR_AGE'] - data['DAYS_EMPLOYED']\n",
    "    # Ratio : Âge de la voiture du demandeur / Ancienneté dans l'emploi à date\n",
    "    # demande\n",
    "    data['CAR_EMPLOYED_RATIO'] = data['OWN_CAR_AGE'] / \\\n",
    "        (data['DAYS_EMPLOYED'] + 0.00001)\n",
    "    # Différence : Âge du demandeur - Âge de la voiture du demandeur\n",
    "    data['CAR_AGE_DIFF'] = data['DAYS_BIRTH'] - data['OWN_CAR_AGE']\n",
    "    # Ratio : Âge de la voiture du demandeur / Âge du demandeur\n",
    "    data['CAR_AGE_RATIO'] = data['OWN_CAR_AGE'] / \\\n",
    "        (data['DAYS_BIRTH'] + 0.00001)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur les contacts\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Somme : téléphone portable? + téléphone professionnel? + téléphone\n",
    "    #         professionnel fixe? + téléphone portable joignable? +\n",
    "    #         adresse de messagerie électronique?\n",
    "    data['FLAG_CONTACTS_SUM'] = data['FLAG_MOBIL'] + data['FLAG_EMP_PHONE'] + \\\n",
    "        data['FLAG_WORK_PHONE'] + data['FLAG_CONT_MOBILE'] + \\\n",
    "        data['FLAG_PHONE'] + data['FLAG_EMAIL']\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur les membres de la famille\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Différence : membres de la famille - enfants (adultes)\n",
    "    data['CNT_NON_CHILDREN'] = data['CNT_FAM_MEMBERS'] - data['CNT_CHILDREN']\n",
    "    # Ratio : nombre d'enfants / Revenu du demandeur\n",
    "    data['CHILDREN_INCOME_RATIO'] = data['CNT_CHILDREN'] / \\\n",
    "        (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    # Ratio : Revenu du demandeur / membres de la famille : revenu par tête\n",
    "    data['PER_CAPITA_INCOME'] = data['AMT_INCOME_TOTAL'] / \\\n",
    "        (data['CNT_FAM_MEMBERS'] + 1)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur la région\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Moyenne : moyenne de notes de la région/ville où vit le client * revenu\n",
    "    # du demandeur\n",
    "    data['REGIONS_INCOME_MOY'] = (data['REGION_RATING_CLIENT'] +\n",
    "                                  data['REGION_RATING_CLIENT_W_CITY']) * data['AMT_INCOME_TOTAL'] / 2\n",
    "    # Max : meilleure note de la région/ville où vit le client\n",
    "    data['REGION_RATING_MAX'] = [max(ele1, ele2) for ele1, ele2 in zip(\n",
    "        data['REGION_RATING_CLIENT'], data['REGION_RATING_CLIENT_W_CITY'])]\n",
    "    # Min : plus faible note de la région/ville où vit le client\n",
    "    data['REGION_RATING_MIN'] = [min(ele1, ele2) for ele1, ele2 in zip(\n",
    "        data['REGION_RATING_CLIENT'], data['REGION_RATING_CLIENT_W_CITY'])]\n",
    "    # Moyenne : des notes de la région et de la ville où vit le client\n",
    "    data['REGION_RATING_MEAN'] = (\n",
    "        data['REGION_RATING_CLIENT'] + data['REGION_RATING_CLIENT_W_CITY']) / 2\n",
    "    # Multipication : note de la région/ note de la ville où vit le client\n",
    "    data['REGION_RATING_MUL'] = data['REGION_RATING_CLIENT'] * \\\n",
    "        data['REGION_RATING_CLIENT_W_CITY']\n",
    "    # Somme : des indicateurs  :\n",
    "    # Indicateur si l'adresse permanente du client ne correspond pas à l'adresse de contact (1=différent ou 0=identique - au niveau de la région)\n",
    "    # Indicateur si l'adresse permanente du client ne correspond pas à l'adresse professionnelle (1=différent ou 0=identique - au niveau de la région)\n",
    "    # Indicateur si l'adresse de contact du client ne correspond pas à l'adresse de travail (1=différent ou 0=identique - au niveau de la région).\n",
    "    # Indicateur si l'adresse permanente du client ne correspond pas à l'adresse de contact (1=différent ou 0=identique - au niveau de la ville)\n",
    "    # Indicateur si l'adresse permanente du client ne correspond pas à l'adresse professionnelle (1=différent ou 0=même - au niveau de la ville).\n",
    "    # Indicateur si l'adresse de contact du client ne correspond pas à\n",
    "    # l'adresse de travail (1=différent ou 0=identique - au niveau de la\n",
    "    # ville).\n",
    "    data['FLAG_REGIONS_SUM'] = data['REG_REGION_NOT_LIVE_REGION'] + \\\n",
    "        data['REG_REGION_NOT_WORK_REGION'] + \\\n",
    "        data['LIVE_REGION_NOT_WORK_REGION'] + \\\n",
    "        data['REG_CITY_NOT_LIVE_CITY'] + \\\n",
    "        data['REG_CITY_NOT_WORK_CITY'] + \\\n",
    "        data['LIVE_CITY_NOT_WORK_CITY']\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur les sources externes : sum, min, multiplication, max, var, scoring\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Somme : somme des scores des 3 sources externes\n",
    "    data['EXT_SOURCE_SUM'] = data[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
    "                                   'EXT_SOURCE_3']].sum(axis=1)\n",
    "    # Moyenne : moyenne des scores des 3 sources externes\n",
    "    data['EXT_SOURCE_MEAN'] = data[['EXT_SOURCE_1', 'EXT_SOURCE_2',\n",
    "                                    'EXT_SOURCE_3']].mean(axis=1)\n",
    "    # Multiplication : des scores des 3 sources externes\n",
    "    data['EXT_SOURCE_MUL'] = data['EXT_SOURCE_1'] * \\\n",
    "        data['EXT_SOURCE_2'] * data['EXT_SOURCE_3']\n",
    "    # Max : Max parmi les 3 scores des 3 sources externes\n",
    "    data['EXT_SOURCE_MAX'] = [max(ele1, ele2, ele3) for ele1, ele2, ele3 in zip(\n",
    "        data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "    # Min : Min parmi les 3 scores des 3 sources externes\n",
    "    data['EXT_SOURCE_MIN'] = [min(ele1, ele2, ele3) for ele1, ele2, ele3 in zip(\n",
    "        data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "    # Variance : variance des scores des 3 sources externes\n",
    "    data['EXT_SOURCE_VAR'] = [np.var([ele1, ele2, ele3]) for ele1, ele2, ele3 in zip(\n",
    "        data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "    # Scoring : scoring des scores des 3 sources externes, score 1 poids 2...\n",
    "    data['WEIGHTED_EXT_SOURCE'] = data.EXT_SOURCE_1 * \\\n",
    "        2 + data.EXT_SOURCE_2 * 3 + data.EXT_SOURCE_3 * 4\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur le bâtiment\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Somme : Informations normalisées sur l'immeuble où vit le demandeur des moyennes\n",
    "    # de la taille de l'appartement, de la surface commune, de la surface habitable,\n",
    "    # de l'âge de l'immeuble, du nombre d'ascenseurs, du nombre d'entrées,\n",
    "    # de l'état de l'immeuble et du nombre d'étages.\n",
    "    data['APARTMENTS_SUM_AVG'] = data['APARTMENTS_AVG'] + data['BASEMENTAREA_AVG'] + data['YEARS_BEGINEXPLUATATION_AVG'] + data[\n",
    "        'YEARS_BUILD_AVG'] + data['ELEVATORS_AVG'] + data['ENTRANCES_AVG'] + data[\n",
    "        'FLOORSMAX_AVG'] + data['FLOORSMIN_AVG'] + data['LANDAREA_AVG'] + data[\n",
    "        'LIVINGAREA_AVG'] + data['NONLIVINGAREA_AVG']\n",
    "    # Somme : Informations normalisées sur l'immeuble où vit le demandeur des modes\n",
    "    # de la taille de l'appartement, de la surface commune, de la surface habitable,\n",
    "    # de l'âge de l'immeuble, du nombre d'ascenseurs, du nombre d'entrées,\n",
    "    # de l'état de l'immeuble et du nombre d'étages.\n",
    "    data['APARTMENTS_SUM_MODE'] = data['APARTMENTS_MODE'] + data['BASEMENTAREA_MODE'] + data['YEARS_BEGINEXPLUATATION_MODE'] + data[\n",
    "        'YEARS_BUILD_MODE'] + data['ELEVATORS_MODE'] + data['ENTRANCES_MODE'] + data[\n",
    "        'FLOORSMAX_MODE'] + data['FLOORSMIN_MODE'] + data['LANDAREA_MODE'] + data[\n",
    "        'LIVINGAREA_MODE'] + data['NONLIVINGAREA_MODE'] + data['TOTALAREA_MODE']\n",
    "    # Somme : Informations normalisées sur l'immeuble où vit le demandeur des médianes\n",
    "    # de la taille de l'appartement, de la surface commune, de la surface habitable,\n",
    "    # de l'âge de l'immeuble, du nombre d'ascenseurs, du nombre d'entrées,\n",
    "    # de l'état de l'immeuble et du nombre d'étages.\n",
    "    data['APARTMENTS_SUM_MEDI'] = data['APARTMENTS_MEDI'] + data['BASEMENTAREA_MEDI'] + data['YEARS_BEGINEXPLUATATION_MEDI'] + data[\n",
    "        'YEARS_BUILD_MEDI'] + data['ELEVATORS_MEDI'] + data['ENTRANCES_MEDI'] + data[\n",
    "        'FLOORSMAX_MEDI'] + data['FLOORSMIN_MEDI'] + data['LANDAREA_MEDI'] + \\\n",
    "        data['NONLIVINGAREA_MEDI']\n",
    "    # Multiplication : somme des moyennes des infos sur le bâtiment * revenu\n",
    "    # du demandeur\n",
    "    data['INCOME_APARTMENT_AVG_MUL'] = data['APARTMENTS_SUM_AVG'] * \\\n",
    "        data['AMT_INCOME_TOTAL']\n",
    "    # Multiplication : somme des modes des infos sur le bâtiment * revenu du\n",
    "    # demandeur\n",
    "    data['INCOME_APARTMENT_MODE_MUL'] = data['APARTMENTS_SUM_MODE'] * \\\n",
    "        data['AMT_INCOME_TOTAL']\n",
    "    # Multiplication : somme des médianes des infos sur le bâtiment * revenu\n",
    "    # du demandeur\n",
    "    data['INCOME_APARTMENT_MEDI_MUL'] = data['APARTMENTS_SUM_MEDI'] * \\\n",
    "        data['AMT_INCOME_TOTAL']\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur les défauts de paiements et les défauts observables\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Somme : nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts observables de 30 DPD (jours de retard) +\n",
    "    #        nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts observables de 60 DPD (jours de retard)\n",
    "    data['OBS_30_60_SUM'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] + \\\n",
    "        data['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "    # Somme : nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement de 30 DPD (jours de retard) +\n",
    "    #        nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement de 60 DPD (jours de retard)\n",
    "    data['DEF_30_60_SUM'] = data['DEF_30_CNT_SOCIAL_CIRCLE'] + \\\n",
    "        data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    # Multiplication : nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts observables de 30 DPD (jours de retard) *\n",
    "    #        nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts observables de 60 DPD (jours de retard)\n",
    "    data['OBS_DEF_30_MUL'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] * \\\n",
    "        data['DEF_30_CNT_SOCIAL_CIRCLE']\n",
    "    # Multiplication : nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement de 30 DPD (jours de retard) *\n",
    "    #        nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement de 60 DPD (jours de retard)\n",
    "    data['OBS_DEF_60_MUL'] = data['OBS_60_CNT_SOCIAL_CIRCLE'] * \\\n",
    "        data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    # Somme : nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement ou des défauts observables avec 30\n",
    "    #         DPD (jours de retard) et 60 DPD.\n",
    "    data['SUM_OBS_DEF_ALL'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] + data['DEF_30_CNT_SOCIAL_CIRCLE'] + \\\n",
    "        data['OBS_60_CNT_SOCIAL_CIRCLE'] + data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "    # Ratio : Montant du crédit du prêt /\n",
    "    #         nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts observables de 30 DPD (jours de retard)\n",
    "    data['OBS_30_CREDIT_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['OBS_30_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    # Ratio : Montant du crédit du prêt /\n",
    "    #         nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts observables de 60 DPD (jours de retard)\n",
    "    data['OBS_60_CREDIT_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['OBS_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    # Ratio : Montant du crédit du prêt /\n",
    "    #         nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement de 30 DPD (jours de retard)\n",
    "    data['DEF_30_CREDIT_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['DEF_30_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "    # Ratio : Montant du crédit du prêt /\n",
    "    #         nombre d'observations de l'environnement social du demandeur\n",
    "    #         avec des défauts de paiement de 60 DPD (jours de retard)\n",
    "    data['DEF_60_CREDIT_RATIO'] = data['AMT_CREDIT'] / \\\n",
    "        (data['DEF_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur les indicateurs des documents fournis ou non\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Toutes les variables DOCUMENT_\n",
    "    cols_flag_doc = [flag for flag in data.columns if 'FLAG_DOC' in flag]\n",
    "    # Somme : tous les indicateurs des documents fournis ou non\n",
    "    data['FLAGS_DOCUMENTS_SUM'] = data[cols_flag_doc].sum(axis=1)\n",
    "    # Moyenne : tous les indicateurs des documents fournis ou non\n",
    "    data['FLAGS_DOCUMENTS_AVG'] = data[cols_flag_doc].mean(axis=1)\n",
    "    # Variance : tous les indicateurs des documents fournis ou non\n",
    "    data['FLAGS_DOCUMENTS_VAR'] = data[cols_flag_doc].var(axis=1)\n",
    "    # Ecart-type : tous les indicateurs des documents fournis ou non\n",
    "    data['FLAGS_DOCUMENTS_STD'] = data[cols_flag_doc].std(axis=1)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Variables sur le détail des modifications du demandeur : jour/heure...\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Somme : nombre de jours avant la demande de changement de téléphone\n",
    "    #         + nombre de jours avant la demande de changement enregistré sur la demande\n",
    "    #         + nombre de jours avant la demande le client où il à\n",
    "    #           changé la pièce d'identité avec laquelle il a demandé le prêt\n",
    "    data['DAYS_DETAILS_CHANGE_SUM'] = data['DAYS_LAST_PHONE_CHANGE'] + \\\n",
    "        data['DAYS_REGISTRATION'] + data['DAYS_ID_PUBLISH']\n",
    "    # Somme : nombre de demandes de renseignements sur le client adressées au Bureau de crédit\n",
    "    # une heure + 1 jour + 1 mois + 3 mois + 1 an et 1 jour avant la demande\n",
    "    data['AMT_ENQ_SUM'] = data['AMT_REQ_CREDIT_BUREAU_HOUR'] + data['AMT_REQ_CREDIT_BUREAU_DAY'] + data['AMT_REQ_CREDIT_BUREAU_WEEK'] + \\\n",
    "        data['AMT_REQ_CREDIT_BUREAU_MON'] + \\\n",
    "            data['AMT_REQ_CREDIT_BUREAU_QRT'] + \\\n",
    "                data['AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "    # Ratio : somme du nombre de demandes de renseignements sur le client adressées au Bureau de crédit\n",
    "    #         une heure + 1 jour + 1 mois + 3 mois + 1 an et 1 jour avant la demande \\\n",
    "    #         Montant du crédit du prêt\n",
    "    data['ENQ_CREDIT_RATIO'] = data['AMT_ENQ_SUM'] / \\\n",
    "        (data['AMT_CREDIT'] + 0.00001)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0c2b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_variables(dataframe, type_var='all'):\n",
    "    df_taille = dataframe.shape[0]\n",
    "\n",
    "    if type_var == 'num':\n",
    "        desc_var = dataframe.select_dtypes(include=[np.number]).describe()\n",
    "    elif type_var == 'cat':\n",
    "        desc_var = dataframe.select_dtypes(exclude=[np.number]).describe()\n",
    "    else:\n",
    "        desc_var = dataframe.describe(include='all')\n",
    "\n",
    "    desc_type = pd.DataFrame(dataframe.dtypes, columns=['type']).T\n",
    "    nb_nan = dataframe.isnull().sum()\n",
    "    pourcentage_nan = (nb_nan / df_taille) * 100\n",
    "    desc_nan = pd.DataFrame([nb_nan, pourcentage_nan], index=['nb_nan', '%_nan']).T\n",
    "    desc_var_transpose = desc_var.T  # Transposer pour aligner avec les autres DataFrame\n",
    "    desc_final = pd.concat([desc_type, desc_nan, desc_var_transpose], axis=1)\n",
    "\n",
    "    return desc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dde3d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_var_num(dataframe, group_var, dict_agg, prefix):\n",
    "    \"\"\"\n",
    "    Aggregates the numeric values in a dataframe.\n",
    "    This can be used to create features for each instance of the grouping variable.\n",
    "    Parameters\n",
    "    --------\n",
    "        dataframe (dataframe): the dataframe to calculate the statistics on\n",
    "        group_var (string): the variable by which to group df\n",
    "        df_name (string): the variable used to rename the columns\n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for \n",
    "            all numeric columns. Each instance of the grouping variable will have \n",
    "            some statistics (mean, min, max, sum ...) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in dataframe:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            dataframe = dataframe.drop(columns=col)\n",
    "\n",
    "    group_ids = dataframe[group_var]\n",
    "    numeric_df = dataframe.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(dict_agg)\n",
    "\n",
    "    # Ajout suffix mean, sum...\n",
    "    agg.columns = ['_'.join(tup).strip().upper()\n",
    "                   for tup in agg.columns.values]\n",
    "\n",
    "    # Ajout du prefix bureau_balance pour avoir une idée du fichier\n",
    "    agg.columns = [prefix + '_' + col\n",
    "                   if col != group_var else col\n",
    "                   for col in agg.columns]\n",
    "\n",
    "    agg.reset_index(inplace=True)\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080ccf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_var_cat(dataframe, group_var, prefix):\n",
    "    '''\n",
    "        Aggregates the categorical features in a child dataframe\n",
    "        for each observation of the parent variable.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        - dataframe        : pandas dataframe\n",
    "                    The dataframe to calculate the value counts for.\n",
    "            \n",
    "        - parent_var : string\n",
    "                    The variable by which to group and aggregate \n",
    "                    the dataframe. For each unique value of this variable, \n",
    "                    the final dataframe will have one row\n",
    "            \n",
    "        - prefix    : string\n",
    "                    Variable added to the front of column names \n",
    "                    to keep track of columns\n",
    "\n",
    "        Return\n",
    "        --------\n",
    "        categorical : pandas dataframe\n",
    "                    A dataframe with aggregated statistics for each observation \n",
    "                    of the parent_var\n",
    "                    The columns are also renamed and columns with duplicate values \n",
    "                    are removed.\n",
    "    '''\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = pd.get_dummies(dataframe.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = dataframe[group_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'count', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (prefix, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    # Remove duplicate columns by values\n",
    "    # _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    # categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a23b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suppr_var_colineaire(dataframe, seuil=0.8):\n",
    "    '''\n",
    "    Récupération de la liste des variables fortement corrélées supérieur\n",
    "    au seuil transmis.\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe : dataframe à analyser, obligatoire.\n",
    "    seuil : le seuil de colinéarité entre les variables (0.8 par défaut).\n",
    "    Returns\n",
    "    -------\n",
    "    cols_corr_a_supp : liste des variables à supprimer.\n",
    "    '''\n",
    "    \n",
    "    # Matrice de corrélation avec valeur absolue pour ne pas avoir à gérer\n",
    "    # les corrélations positives et négatives séparément\n",
    "    corr = dataframe.corr().abs()\n",
    "    # On ne conserve que la partie supérieur à la diagonale pour n'avoir\n",
    "    # qu'une seule fois les corrélations prisent en compte (symétrie axiale)\n",
    "    corr_triangle = corr.where(np.triu(np.ones(corr.shape), k=1)\n",
    "                               .astype(np.bool_))\n",
    "    \n",
    "    # Variables avec un coef de Pearson > 0.8?\n",
    "    cols_corr_a_supp = [var for var in corr_triangle.columns\n",
    "                        if any(corr_triangle[var] > seuil)]\n",
    "    print(f'{len(cols_corr_a_supp)} variables fortement corrélées à supprimer :\\n')\n",
    "    for var in cols_corr_a_supp:\n",
    "        print(var)\n",
    "        \n",
    "    return cols_corr_a_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99f86d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_moy_par_pret(dataframe, group_var, prefix):\n",
    "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        dataframe (dataframe): \n",
    "            the dataframe to calculate the statistics on\n",
    "        group_var (string): \n",
    "            the variable by which to group df\n",
    "        prefix (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for \n",
    "            all numeric columns. Each instance of the grouping variable will have \n",
    "            the statistics (mean, min, max, sum; currently supported) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in dataframe:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            dataframe = dataframe.drop(columns = col)\n",
    "            \n",
    "    group_ids = dataframe[group_var]\n",
    "    numeric_df = dataframe.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['mean']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (prefix, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "    \n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25e6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Plots 15 most important features and the cumulative importance of features.\n",
    "    Prints the number of features needed to reach threshold cumulative importance.\n",
    "    Source : \n",
    "    https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection\n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe of feature importances. Columns must be feature and importance\n",
    "    threshold : float, default = 0.9\n",
    "        Threshold for prining information about cumulative importances\n",
    "    Return\n",
    "    --------\n",
    "    df : dataframe\n",
    "        Dataframe ordered by feature importances with a normalized column (sums to 1)\n",
    "        and a cumulative importance column    \n",
    "    \"\"\"\n",
    "    \n",
    "    plt.rcParams['font.size'] = 18\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 12))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:30]))), \n",
    "            df['importance_normalized'].head(30), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:30]))))\n",
    "    ax.set_yticklabels(df['feature'].head(30))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Importance normalisée'); plt.title('Features Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    # Cumulative importance plot\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(list(range(len(df))), df['cumulative_importance'], 'r-')\n",
    "    plt.xlabel('Nombre de variables'); plt.ylabel('Cumulative Importance'); \n",
    "    plt.title('Cumulative Feature Importance');\n",
    "    plt.show();\n",
    "    \n",
    "    importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "    print('%d variables nécessaires pour %0.2f de cumulative importance' % (importance_index + 1, threshold))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ab8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_zero_importance_features(train, train_labels, iterations = 2):\n",
    "    \"\"\"\n",
    "    Identify zero importance features in a training dataset based on the \n",
    "    feature importances from a gradient boosting model. \n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    train : dataframe\n",
    "        Training features\n",
    "        \n",
    "    train_labels : np.array\n",
    "        Labels for training data\n",
    "        \n",
    "    iterations : integer, default = 2\n",
    "        Number of cross validation splits to use for determining feature importances\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty array to hold feature importances\n",
    "    feature_importances = np.zeros(train.shape[1])\n",
    "\n",
    "    # Create the model with several hyperparameters\n",
    "    model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')\n",
    "    \n",
    "    # Fit the model multiple times to avoid overfitting\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # Split into training and validation set\n",
    "        train_features, valid_features, train_y, valid_y = train_test_split(train, train_labels, test_size = 0.25, random_state = i)\n",
    "\n",
    "        # Train using early stopping\n",
    "        model.fit(train_features, train_y, eval_set = [(valid_features, valid_y)], \n",
    "                  eval_metric = 'auc')\n",
    "\n",
    "        # Record the feature importances\n",
    "        feature_importances += model.feature_importances_ / iterations\n",
    "    \n",
    "    feature_importances = pd.DataFrame({'feature': list(train.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "    \n",
    "    # Find the features with zero importance\n",
    "    zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "    print('\\nThere are %d features with 0.0 importance' % len(zero_features))\n",
    "    \n",
    "    return zero_features, feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936b1e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracer_features_importance(dataframe, df_features_importance, jeu, methode):\n",
    "    \"\"\"\n",
    "    Affiche l'étape puis nombre de lignes et de variables pour le dataframe transmis\n",
    "    Parameters\n",
    "    ----------\n",
    "    @param IN : dataframe : DataFrame, obligatoire\n",
    "                df_features_importance : dataframe de suivi des dimensions,\n",
    "                                         obligatoire\n",
    "                jeu : jeu de données train_set, train set avec imputation 1...\n",
    "                methode : titre du modèle de feature sélection\n",
    "    @param OUT : dataframe de suivi des dimensions\n",
    "    \"\"\"\n",
    "    # Nombre de variables retenues lors de la feature selection\n",
    "    n_features = dataframe.shape[0]\n",
    "    print(f'{jeu} - {methode} : {n_features} variables importantes conservées')\n",
    "\n",
    "    # Création d'un DataFrame pour les nouvelles données\n",
    "    new_data = pd.DataFrame({\n",
    "        'Jeu_données': [jeu],\n",
    "        'Méthode': [methode],\n",
    "        'Nb_var_importante': [n_features]\n",
    "    })\n",
    "\n",
    "    # Concaténation du nouveau DataFrame avec l'existant\n",
    "    df_features_importance = pd.concat([df_features_importance, new_data], ignore_index=True)\n",
    "\n",
    "    # Suivi dimensions\n",
    "    return df_features_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240a74b0-e7b9-415c-92e8-b10b45c6ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_importance(model, x_test, y_test, figsize=(6, 6)):\n",
    "    '''\n",
    "    Affiche les SHAPE VALUES.\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: le modèle de machine learning, obligatoire\n",
    "    x_test :le jeu de test de la matrice X, obligatoire\n",
    "    y_test :le jeu de test de la target, obligatoire\n",
    "    Returns\n",
    "    -------\n",
    "    perm_importance : permutation importance\n",
    "    '''\n",
    "    perm_importance = permutation_importance(model, x_test, y_test)\n",
    "\n",
    "    sorted_idx = perm_importance.importances_mean.argsort()\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(x_test.columns[sorted_idx],\n",
    "             perm_importance.importances_mean[sorted_idx])\n",
    "    plt.xlabel(\"Permutation Importance (%)\")\n",
    "    plt.show()    \n",
    "    \n",
    "    return perm_importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4db7bb-0c22-4687-ae4f-d17c6ef0e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_variables_plages(\n",
    "        dataframe, variable, liste_bins):\n",
    "    \"\"\"\n",
    "    Retourne les plages des pourcentages des valeurs pour le découpage transmis\n",
    "    Parameters\n",
    "    ----------\n",
    "    @param IN : dataframe : DataFrame, obligatoire\n",
    "                variable : variable à découper obligatoire\n",
    "                liste_bins: liste des découpages facultatif int ou pintervallindex\n",
    "    @param OUT : dataframe des plages de nan\n",
    "    \"\"\"\n",
    "    nb_lignes = len(dataframe[variable])\n",
    "    s_gpe_cut = pd.cut(\n",
    "        dataframe[variable],\n",
    "        bins=liste_bins).value_counts().sort_index()\n",
    "    df_cut = pd.DataFrame({'Plage': s_gpe_cut.index,\n",
    "                           'nb_données': s_gpe_cut.values})\n",
    "    df_cut['%_données'] = [\n",
    "        (row * 100) / nb_lignes for row in df_cut['nb_données']]\n",
    "\n",
    "    return df_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0215abf-18de-49df-8633-f7d93829ebf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet7_OCR",
   "language": "python",
   "name": "projet7_ocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
